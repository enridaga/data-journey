digraph { 
"numpy" -> "applying-text-mining.ipynb" [label = "importedBy"]
"np(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"np(0)" -> "numpy" [label = "assignedFrom"]
"pandas" -> "applying-text-mining.ipynb" [label = "importedBy"]
"pd(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"pd(0)" -> "pandas" [label = "assignedFrom"]
"os" -> "applying-text-mining.ipynb" [label = "importedBy"]
"os(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"os(0)" -> "os" [label = "assignedFrom"]
"print[0]" -> "os(0)" [label = "print"]
"../input(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[1]" -> "../input(0)" [label = "print"]
"No woman no cry(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"text(0)$0" -> "No woman no cry(0)" [label = "assignedFrom"]
"length of text: (0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "length of text: (0)" [label = "print"]
"len(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[1]" -> "len(0)" [label = "print"]
"print[2]" -> "text(0)$0" [label = "print"]
"splitted_text(0)$0" -> "text(0)$0" [label = "split"]
"Splitted text: (0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "Splitted text: (0)" [label = "print"]
"print[1]" -> "splitted_text(0)$0" [label = "print"]
"word(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"specific_words(0)$0" -> "word(0)" [label = "assignedFrom"]
"specific_words(0)$0" -> "word(0)" [label = "assignedFrom"]
"specific_words(0)$0" -> "splitted_text(0)$0" [label = "assignedFrom"]
"specific_words(0)$0" -> "len(0)" [label = "assignedFrom"]
"specific_words(0)$0" -> "word(0)" [label = "assignedFrom"]
"2(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"specific_words(0)$0" -> "2(0)" [label = "assignedFrom"]
"Words which are more than 3 letter: (0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "Words which are more than 3 letter: (0)" [label = "print"]
"print[1]" -> "specific_words(0)$0" [label = "print"]
"capital_words(0)$0" -> "word(0)" [label = "assignedFrom"]
"capital_words(0)$0" -> "word(0)" [label = "assignedFrom"]
"capital_words(0)$0" -> "splitted_text(0)$0" [label = "assignedFrom"]
"capital_words(0)$0" -> "word(0)" [label = "assignedFrom"]
"Capitalized words: (0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "Capitalized words: (0)" [label = "print"]
"print[1]" -> "capital_words(0)$0" [label = "print"]
"words_end_with_o(0)$0" -> "word(0)" [label = "assignedFrom"]
"words_end_with_o(0)$0" -> "word(0)" [label = "assignedFrom"]
"words_end_with_o(0)$0" -> "splitted_text(0)$0" [label = "assignedFrom"]
"words_end_with_o(0)$0" -> "word(0)" [label = "assignedFrom"]
"o(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"words_end_with_o(0)$0" -> "o(0)" [label = "assignedFrom"]
"words end with o: (0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "words end with o: (0)" [label = "print"]
"print[1]" -> "words_end_with_o(0)$0" [label = "print"]
"words_start_with_w(0)$0" -> "word(0)" [label = "assignedFrom"]
"words_start_with_w(0)$0" -> "word(0)" [label = "assignedFrom"]
"words_start_with_w(0)$0" -> "splitted_text(0)$0" [label = "assignedFrom"]
"words_start_with_w(0)$0" -> "word(0)" [label = "assignedFrom"]
"w(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"words_start_with_w(0)$0" -> "w(0)" [label = "assignedFrom"]
"words start with w: (0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "words start with w: (0)" [label = "print"]
"print[1]" -> "words_start_with_w(0)$0" [label = "print"]
"unique words: (0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "unique words: (0)" [label = "print"]
"set(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[1]" -> "set(0)" [label = "print"]
"print[2]" -> "splitted_text(0)$0" [label = "print"]
"lowercase_text(0)$0" -> "word(0)" [label = "assignedFrom"]
"lowercase_text(0)$0" -> "word(0)" [label = "assignedFrom"]
"lowercase_text(0)$0" -> "splitted_text(0)$0" [label = "assignedFrom"]
"print[0]" -> "unique words: (0)" [label = "print"]
"print[1]" -> "set(0)" [label = "print"]
"print[2]" -> "lowercase_text(0)$0" [label = "print"]
"Is w letter in woman word:(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "Is w letter in woman word:(0)" [label = "print"]
"print[1]" -> "w(0)" [label = "print"]
"woman(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[2]" -> "woman(0)" [label = "print"]
"Is word uppercase:(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "Is word uppercase:(0)" [label = "print"]
"WOMAN(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[1]" -> "WOMAN(0)" [label = "print"]
"Is word lowercase:(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "Is word lowercase:(0)" [label = "print"]
"cry(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[1]" -> "cry(0)" [label = "print"]
"Is word made of by digits: (0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "Is word made of by digits: (0)" [label = "print"]
"12345(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[1]" -> "12345(0)" [label = "print"]
"00000000No cry: (0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "00000000No cry: (0)" [label = "print"]
"00000000No cry(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[1]" -> "00000000No cry(0)" [label = "print"]
"0(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[2]" -> "0(0)" [label = "print"]
"Find particular letter from back: (0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "Find particular letter from back: (0)" [label = "print"]
"No cry no(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[1]" -> "No cry no(0)" [label = "print"]
"print[2]" -> "o(0)" [label = "print"]
"print[0]" -> "Find particular letter from back: (0)" [label = "print"]
"print[1]" -> "No cry no(0)" [label = "print"]
"print[2]" -> "o(0)" [label = "print"]
"Replace o with 3 (0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "Replace o with 3 (0)" [label = "print"]
"print[1]" -> "No cry no(0)" [label = "print"]
"print[2]" -> "o(0)" [label = "print"]
"3(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[3]" -> "3(0)" [label = "print"]
"Each letter: (0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "Each letter: (0)" [label = "print"]
"list(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[1]" -> "list(0)" [label = "print"]
"No cry(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[2]" -> "No cry(0)" [label = "print"]
"    Be fair and tolerant    (0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"text1(0)$0" -> "    Be fair and tolerant    (0)" [label = "assignedFrom"]
"Split text: (0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "Split text: (0)" [label = "print"]
"print[1]" -> "text1(0)$0" [label = "print"]
" (0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[2]" -> " (0)" [label = "print"]
"Cleaned text: (0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "Cleaned text: (0)" [label = "print"]
"print[1]" -> "text1(0)$0" [label = "print"]
"print[2]" -> " (0)" [label = "print"]
"../input/religious-and-philosophical-texts/35895-0.txt(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"f(0)$0" -> "../input/religious-and-philosophical-texts/35895-0.txt(0)" [label = "open"]
"r(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"f(0)$0" -> "r(0)" [label = "open"]
"print[0]" -> "f(0)$0" [label = "print"]
"text3(0)$0" -> "f(0)$0" [label = "read"]
"Length of text: (0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "Length of text: (0)" [label = "print"]
"print[1]" -> "len(0)" [label = "print"]
"print[2]" -> "text3(0)$0" [label = "print"]
"lines(0)$0" -> "text3(0)$0" [label = "splitlines"]
"Number of lines: (0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "Number of lines: (0)" [label = "print"]
"print[1]" -> "len(0)" [label = "print"]
"print[2]" -> "lines(0)$0" [label = "print"]
"data(0)$0" -> "pd(0)" [label = "read_csv"]
"../input/ben-hamners-tweets/benhamner.csv(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"data(0)$0" -> "../input/ben-hamners-tweets/benhamner.csv(0)" [label = "read_csv"]
"latin-1(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"data(0)$0" -> "latin-1(0)" [label = "read_csv"]
"data(0)$1" -> "data(0)$0" [label = "head"]
"In his tweets, the rate of occuring kaggle word is: (0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "In his tweets, the rate of occuring kaggle word is: (0)" [label = "print"]
"sum(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[1]" -> "sum(0)" [label = "print"]
"print[2]" -> "data(0)$1" [label = "print"]
"kaggle(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[3]" -> "kaggle(0)" [label = "print"]
"print[4]" -> "len(0)" [label = "print"]
"print[5]" -> "data(0)$1" [label = "print"]
"text(0)$1" -> "data(0)$1" [label = "assignedFrom"]
"1(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"text(0)$1" -> "1(0)" [label = "assignedFrom"]
"print[0]" -> "text(0)$1" [label = "print"]
"re" -> "applying-text-mining.ipynb" [label = "importedBy"]
"re(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"re(0)" -> "re" [label = "assignedFrom"]
"callouts(0)$0" -> "word(0)" [label = "assignedFrom"]
"callouts(0)$0" -> "word(0)" [label = "assignedFrom"]
"callouts(0)$0" -> "text(0)$1" [label = "assignedFrom"]
"callouts(0)$0" -> " (0)" [label = "assignedFrom"]
"callouts(0)$0" -> "re(0)" [label = "assignedFrom"]
"@[A-Za-z0-9_]+(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"callouts(0)$0" -> "@[A-Za-z0-9_]+(0)" [label = "assignedFrom"]
"callouts(0)$0" -> "word(0)" [label = "assignedFrom"]
"callouts: (0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "callouts: (0)" [label = "print"]
"print[1]" -> "callouts(0)$0" [label = "print"]
"callouts1(0)$0" -> "word(0)" [label = "assignedFrom"]
"callouts1(0)$0" -> "word(0)" [label = "assignedFrom"]
"callouts1(0)$0" -> "text(0)$1" [label = "assignedFrom"]
"callouts1(0)$0" -> " (0)" [label = "assignedFrom"]
"callouts1(0)$0" -> "re(0)" [label = "assignedFrom"]
"@\w+(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"callouts1(0)$0" -> "@\w+(0)" [label = "assignedFrom"]
"callouts1(0)$0" -> "word(0)" [label = "assignedFrom"]
"print[0]" -> "callouts: (0)" [label = "print"]
"print[1]" -> "callouts1(0)$0" [label = "print"]
"print[0]" -> "re(0)" [label = "print"]
"[w](0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[1]" -> "[w](0)" [label = "print"]
"print[2]" -> "text(0)$1" [label = "print"]
"print[0]" -> "re(0)" [label = "print"]
"[^w](0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[1]" -> "[^w](0)" [label = "print"]
"print[2]" -> "text(0)$1" [label = "print"]
"15-10-2000
09/10/2005
15-05-1999
05/05/99

05/05/199

05/05/9(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"date(0)$0" -> "15-10-2000
09/10/2005
15-05-1999
05/05/99

05/05/199

05/05/9(0)" [label = "assignedFrom"]
"re(0)$0" -> "re(0)" [label = "findall"]
"\d{1,2}[/-]\d{1,2}[/-]\d{1,4}(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"re(0)$0" -> "\d{1,2}[/-]\d{1,2}[/-]\d{1,4}(0)" [label = "findall"]
"re(0)$0" -> "date(0)$0" [label = "findall"]
"nltk" -> "applying-text-mining.ipynb" [label = "importedBy"]
"nlp(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"nlp(0)" -> "nltk" [label = "assignedFrom"]
"text(0)$2" -> "data(0)$1" [label = "assignedFrom"]
"text(0)$2" -> "1(0)" [label = "assignedFrom"]
"splitted(0)$0" -> "text(0)$2" [label = "split"]
"splitted(0)$0" -> " (0)" [label = "split"]
"number of words: (0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "number of words: (0)" [label = "print"]
"print[1]" -> "len(0)" [label = "print"]
"print[2]" -> "splitted(0)$0" [label = "print"]
"text(0)$3" -> "data(0)$1" [label = "assignedFrom"]
"text(0)$3" -> "1(0)" [label = "assignedFrom"]
"number of unique words: (0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "number of unique words: (0)" [label = "print"]
"print[1]" -> "len(0)" [label = "print"]
"print[2]" -> "set(0)" [label = "print"]
"print[3]" -> "splitted(0)$0" [label = "print"]
"first 5 unique words: (0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "first 5 unique words: (0)" [label = "print"]
"print[1]" -> "list(0)" [label = "print"]
"print[2]" -> "set(0)" [label = "print"]
"print[3]" -> "splitted(0)$0" [label = "print"]
"5(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[4]" -> "5(0)" [label = "print"]
"dist(0)$0" -> "nlp(0)" [label = "FreqDist"]
"dist(0)$0" -> "splitted(0)$0" [label = "FreqDist"]
"frequency of words: (0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "frequency of words: (0)" [label = "print"]
"print[1]" -> "dist(0)$0" [label = "print"]
"words in text: (0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "words in text: (0)" [label = "print"]
"print[1]" -> "dist(0)$0" [label = "print"]
"the word box is occured how many times:(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "the word box is occured how many times:(0)" [label = "print"]
"print[1]" -> "dist(0)$0" [label = "print"]
"box(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[2]" -> "box(0)" [label = "print"]
"task Tasked tasks tasking(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"words(0)$0" -> "task Tasked tasks tasking(0)" [label = "assignedFrom"]
"words_list(0)$0" -> "words(0)$0" [label = "split"]
"words_list(0)$0" -> " (0)" [label = "split"]
"normalized words: (0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "normalized words: (0)" [label = "print"]
"print[1]" -> "words_list(0)$0" [label = "print"]
"porter_stemmer(0)$0" -> "nlp(0)" [label = "PorterStemmer"]
"roots(0)$0" -> "porter_stemmer(0)$0" [label = "assignedFrom"]
"each(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"roots(0)$0" -> "each(0)" [label = "assignedFrom"]
"roots(0)$0" -> "each(0)" [label = "assignedFrom"]
"roots(0)$0" -> "words_list(0)$0" [label = "assignedFrom"]
"roots of task Tasked tasks tasking: (0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "roots of task Tasked tasks tasking: (0)" [label = "print"]
"print[1]" -> "roots(0)$0" [label = "print"]
"[<_ast.Constant object at 0x104fae3d0>, <_ast.Constant object at 0x104fae400>, <_ast.Constant object at 0x104fae430>, <_ast.Constant object at 0x104fae460>, <_ast.Constant object at 0x104fae490>](0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"stemming_word_list(0)$0" -> "[<_ast.Constant object at 0x104fae3d0>, <_ast.Constant object at 0x104fae400>, <_ast.Constant object at 0x104fae430>, <_ast.Constant object at 0x104fae460>, <_ast.Constant object at 0x104fae490>](0)" [label = "assignedFrom"]
"porter_stemmer(0)$1" -> "nlp(0)" [label = "PorterStemmer"]
"roots(0)$1" -> "porter_stemmer(0)$1" [label = "assignedFrom"]
"roots(0)$1" -> "each(0)" [label = "assignedFrom"]
"roots(0)$1" -> "each(0)" [label = "assignedFrom"]
"roots(0)$1" -> "stemming_word_list(0)$0" [label = "assignedFrom"]
"result of stemming: (0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "result of stemming: (0)" [label = "print"]
"print[1]" -> "roots(0)$1" [label = "print"]
"lemma(0)$0" -> "nlp(0)" [label = "WordNetLemmatizer"]
"lemma_roots(0)$0" -> "lemma(0)$0" [label = "assignedFrom"]
"lemma_roots(0)$0" -> "each(0)" [label = "assignedFrom"]
"lemma_roots(0)$0" -> "each(0)" [label = "assignedFrom"]
"lemma_roots(0)$0" -> "stemming_word_list(0)$0" [label = "assignedFrom"]
"result of lemmatization: (0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "result of lemmatization: (0)" [label = "print"]
"print[1]" -> "lemma_roots(0)$0" [label = "print"]
"You’re in the right place!(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"text_t(0)$0" -> "You’re in the right place!(0)" [label = "assignedFrom"]
"split the sentece: (0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "split the sentece: (0)" [label = "print"]
"print[1]" -> "text_t(0)$0" [label = "print"]
"print[2]" -> " (0)" [label = "print"]
"tokenize with nltk: (0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "tokenize with nltk: (0)" [label = "print"]
"print[1]" -> "nlp(0)" [label = "print"]
"print[2]" -> "text_t(0)$0" [label = "print"]
"data(0)$2" -> "pd(0)" [label = "read_csv"]
"../input/twitter-user-gender-classification/gender-classifier-DFE-791531.csv(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"data(0)$2" -> "../input/twitter-user-gender-classification/gender-classifier-DFE-791531.csv(0)" [label = "read_csv"]
"latin1(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"data(0)$2" -> "latin1(0)" [label = "read_csv"]
"data(0)$3" -> "pd(0)" [label = "concat"]
"[<_ast.Attribute object at 0x104fa3400>, <_ast.Attribute object at 0x104fa3460>](0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"data(0)$3" -> "[<_ast.Attribute object at 0x104fa3400>, <_ast.Attribute object at 0x104fa3460>](0)" [label = "concat"]
"data(0)$3" -> "1(0)" [label = "concat"]
"data(0)$4" -> "data(0)$3" [label = "dropna"]
"data(0)$5" -> "data(0)$4" [label = "assignedFrom"]
"data(0)$5" -> "each(0)" [label = "assignedFrom"]
"female(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"data(0)$5" -> "female(0)" [label = "assignedFrom"]
"data(0)$5" -> "1(0)" [label = "assignedFrom"]
"data(0)$5" -> "0(0)" [label = "assignedFrom"]
"data(0)$5" -> "each(0)" [label = "assignedFrom"]
"data(0)$5" -> "data(0)$5" [label = "assignedFrom"]
"sklearn.feature_extraction.text" -> "applying-text-mining.ipynb" [label = "importedBy"]
"CountVectorizer" -> "sklearn.feature_extraction.text" [label = "importedBy"]
"CountVectorizer(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"CountVectorizer(0)" -> "CountVectorizer" [label = "assignedFrom"]
"150(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"max_features(0)$0" -> "150(0)" [label = "assignedFrom"]
"english(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"count_vectorizer(0)$0" -> "english(0)" [label = "CountVectorizer"]
"count_vectorizer(0)$0" -> "max_features(0)$0" [label = "CountVectorizer"]
"sparce_matrix(0)$0" -> "count_vectorizer(0)$0" [label = "toarray"]
"review_list(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"sparce_matrix(0)$0" -> "review_list(0)" [label = "toarray"]
"Most used {} words: {}(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"print[0]" -> "Most used {} words: {}(0)" [label = "print"]
"print[1]" -> "max_features(0)$0" [label = "print"]
"print[2]" -> "count_vectorizer(0)$0" [label = "print"]
"y(0)$0" -> "data(0)$5" [label = "assignedFrom"]
"y(0)$0" -> "0(0)" [label = "assignedFrom"]
"sklearn.model_selection" -> "applying-text-mining.ipynb" [label = "importedBy"]
"train_test_split" -> "sklearn.model_selection" [label = "importedBy"]
"train_test_split(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"train_test_split(0)" -> "train_test_split" [label = "assignedFrom"]
"x_train(0)$0" -> "sparce_matrix(0)$0" [label = "train_test_split"]
"x_test(0)$0" -> "sparce_matrix(0)$0" [label = "train_test_split"]
"y_train(0)$0" -> "sparce_matrix(0)$0" [label = "train_test_split"]
"y_test(0)$0" -> "sparce_matrix(0)$0" [label = "train_test_split"]
"x_train(0)$0" -> "y(0)$0" [label = "train_test_split"]
"x_test(0)$0" -> "y(0)$0" [label = "train_test_split"]
"y_train(0)$0" -> "y(0)$0" [label = "train_test_split"]
"y_test(0)$0" -> "y(0)$0" [label = "train_test_split"]
"0.1(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"x_train(0)$0" -> "0.1(0)" [label = "train_test_split"]
"x_test(0)$0" -> "0.1(0)" [label = "train_test_split"]
"y_train(0)$0" -> "0.1(0)" [label = "train_test_split"]
"y_test(0)$0" -> "0.1(0)" [label = "train_test_split"]
"x_train(0)$0" -> "0(0)" [label = "train_test_split"]
"x_test(0)$0" -> "0(0)" [label = "train_test_split"]
"y_train(0)$0" -> "0(0)" [label = "train_test_split"]
"y_test(0)$0" -> "0(0)" [label = "train_test_split"]
"sklearn.naive_bayes" -> "applying-text-mining.ipynb" [label = "importedBy"]
"GaussianNB" -> "sklearn.naive_bayes" [label = "importedBy"]
"GaussianNB(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"GaussianNB(0)" -> "GaussianNB" [label = "assignedFrom"]
"nb(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"nb(0)$0" -> "nb(0)" [label = "fit"]
"nb(0)$0" -> "sparce_matrix(0)$0" [label = "fit"]
"nb(0)$0" -> "y(0)$0" [label = "fit"]
"y_pred(0)$0" -> "nb(0)$0" [label = "predict"]
"y_pred(0)$0" -> "sparce_matrix(0)$0" [label = "predict"]
"sklearn.metrics" -> "applying-text-mining.ipynb" [label = "importedBy"]
"confusion_matrix" -> "sklearn.metrics" [label = "importedBy"]
"confusion_matrix(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"confusion_matrix(0)" -> "confusion_matrix" [label = "assignedFrom"]
"cm(0)$0" -> "y(0)$0" [label = "confusion_matrix"]
"cm(0)$0" -> "y_pred(0)$0" [label = "confusion_matrix"]
"matplotlib.pyplot" -> "applying-text-mining.ipynb" [label = "importedBy"]
"plt(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"plt(0)" -> "matplotlib.pyplot" [label = "assignedFrom"]
"seaborn" -> "applying-text-mining.ipynb" [label = "importedBy"]
"sns(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"sns(0)" -> "seaborn" [label = "assignedFrom"]
"f(0)$1" -> "plt(0)" [label = "subplots"]
"ax(0)$0" -> "plt(0)" [label = "subplots"]
"f(0)$1" -> "5(0)" [label = "subplots"]
"ax(0)$0" -> "5(0)" [label = "subplots"]
"f(0)$1" -> "5(0)" [label = "subplots"]
"ax(0)$0" -> "5(0)" [label = "subplots"]
"sns(0)$0" -> "sns(0)" [label = "heatmap"]
"sns(0)$0" -> "cm(0)$0" [label = "heatmap"]
"plt(0)$0" -> "plt(0)" [label = "show"]
"plt(0)$1" -> "plt(0)$0" [label = "savefig"]
"graph.png(0)" -> "applying-text-mining.ipynb" [label = "appearsIn"]
"plt(0)$1" -> "graph.png(0)" [label = "savefig"]
}